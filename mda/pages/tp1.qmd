---
title: "Data Classification"
---

```{r}
#| echo: false
knitr::opts_chunk$set(message = NA)
```

To do:

- [x] Justify scaling  
- [x] Describe classification methods (ward2 etc)  
- [ ] Justify number of clusters  
- [ ] More complete plots

## Introduction

In this tutorial, we will be applying unsupervised classification techniques to
the independent variables in our dataset. The dataset comprises protein pockets
characterized by 18 descriptors, which serve as the independent variables. The
objective is to evaluate the druggability of those pockets.

## Preparing the workspace

### Loading libraries

We first load the libraries necessary to our project:

- [factoextra](https://rpkgs.datanovia.com/factoextra/index.html)  

> an R package making easy to extract and visualize the output of exploratory
> multivariate data analyses

- [ggplot2](https://ggplot2.tidyverse.org/)

> a system for declaratively creating graphics, based on The Grammar of
> Graphics.

```{r}
#| code-fold: false
library(factoextra)
library(tidyverse)
library(reshape2)
library(ggplot2)
library(plotly)
```

### Loading data

```{r}
#| code-fold: false
load('../data/X.Rdata')
```

## Data pre-processing

In classification, scalling the data is a key pre-processing step. A wise choice
of the technique must be done depending on the dateset (is it homogeneous or
heterogeneous ?). Ensuring the features vary in the same range can improve the
performance of the classification of the dataset, as well as better handling the
outliers. However if not adapted to the classification, the scalling technique
can be detrimental to its performance @deamorimChoiceScalingTechnique2023.

Here are two boxplots highlighting how the weight of one feature can dominate
the others:

::: {.panel-tabset}

# Raw data

```{r}
melted_x <- melt(X, variable.name = "Descriptors")
plot_ly(melted_x, x = ~Descriptors, y = ~value, type = "box")
```

# Scaled data

::: {.callout-warning}
Be careful scale() returns a matrix with one column for the row names, one for 
the column names and one for the values. Workarounds could be a transformation 
into a *data.frame* or the column targetting.
:::

```{r}
scaled_x <- scale(X)
scaled_mlt_x <- melt(as.data.frame(scaled_x), variable.name = "Descriptors")
plot_ly(scaled_mlt_x, x = ~Descriptors, y = ~value, type = "box")
```

:::

## Hierarchical classification

This non-supervised technique is used to organize and categorize data into a
hierarchical structure. The principle behind it is the creation of a system of
nested categories. Organizing data in such way makes data easier to read,
navigate and understand. It is an easy and fast way to get familiar with an
unknown dataset.

### Computing the distance matrix

We performed a distance-wise classification between each pocket of our dataset.
We used the base R `dist()` function to do so which uses the Euclidian distance
metric by default. As our dataset is contstitued of continuous variables with
[scaled]{style="text-decoration: underline;"} numerical values. It is the most
commonly used metric as it is very intuitive and easily interpretable.  

```{r}
# Computation of the distance matrix
mat_dist <- dist(scaled_x, method = "euclidian")
```

### Clustering the data

The clustering is performed using the base R `hclust()` function which
agglomerates our datased based on the distance dissimilarity. The default
*complete linkage* method will produce well defined contours for the generated
clusters but not necessarily compact repartition inside the cluster. Here we
will prefer the *Ward.D* method as it produces more dense clusters while keeping
them as far apart as possible.

```{r}
# Computation of the clusters
x_classif <- hclust(mat_dist, method = "ward.D")

# Plot the dendogram
plot(x_classif, labels = FALSE, xlab = "", sub = "")
```

We see that the resulting dendogram presents many subdivisions, going from one
cluster containing the whole dataset to one cluster per entry. Depending on the
study, on the properties that one wants to show, the clustering can be stopped
at a selected level.

### Choosing the number of clusters

To determine the optimal number of clusters we will use the `factoextra` package
function `fviz_nbclust()`. Before fine-tuning the parameters, a theory review is
necessary and luckily, the `factoextra` documentation provides a
[handy explanation](https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/).

#### The *elbow* method

We will first use the *elbow* method which sets the within-cluster sum of square
(WSS), in other words the compacity of the cluster, against the number of
clusters. By plotting this, we aim to find a *bend* that corresponds to a minor
decrease of the WSS meaning the compacity doesn't improve much beyond a certain
number of clusters.

```{r}
# Choose FUNcluster depending on the type of clustering you want to perform,
# here hcut corresponds to hierarchical clustering
fviz_nbclust(scaled_x, method = "wss", FUNcluster=hcut)
```

In the generated plot, we do not a see proper bending, meaning that the *elbow*
method couldn't provide an optimized criterion to choose the optimal number of
clusters. We can estimate an optimal number of clusters between 3 and 5 but
testing other methods will be necessary to ensure it.

#### The *silhouette* method

This method measures the quality of the clustering by assessing how well an
object fit into its assigned cluster. It calculates a silhouette coefficient
which is the difference between the *inter-cluster* distance and the
*intra-cluster* distance, higher values indicating a good clustering and
therefore the optimal number of clusters.

```{r}
# Optimal number of clusters using the silhouette method
fviz_nbclust(scaled_x, method = "silhouette", FUNcluster=hcut)
```

This method gave us more usable results, indicating an optimal number of
clusters of 3 or 5, which is in line with the results obtained above. Another
validation would be usefull to ensure 3 and 5 are really the optimal number of
clusters and luckily, another method can be used with this package: the *gap*
method.

#### The *gap* method

This method determines the optimal number of clusters in a dataset by comparing
the within-cluster dispersion of the dataset to a reference uniform
distribution. For each $k$, a gap representing the distance of the cluster to
the reference is calculated, higher values indicating that the clustering
structure is far from a random distribution fo points.

```{r}
# Optimal number of clusters using the gap method
fviz_nbclust(scaled_x, method = "gap_stat", FUNcluster = hcut)
```

We can see this time that the optimal number of clusters is of 3.

#### Comparison of the methods and choice for our dataset

While the *elbow* method gives the optimal number of clusters through a visual
inspection of the plot, which can be subjective, the *silhouette* and the *gap*
methods give more quantitative results thus being more objective and more
reliable.

These two last showed that the optimal number of clusters is either 3 or 5, the
results being very similar. The choice now depends on the dataset and the
relevance to produce more clusters or not. Our goal being determining the
druggability of protein pockets, it might not be necessary to split the dataset
into many clusters as the properties of those cavities might not highly differ
in general so we will continue with an optimal number of cluster of 3.

### Retrieving the clusters

We can now see visualize our clusters in the dendogram. 

```{r}
# Visualization of the clusters in the dendogram
plot(x_classif, labels = FALSE)
rect.hclust(x_classif, k=3)
```

We can also plot our clusters either by performing a Multidimensional Scaling
(MDS) or by using `fviz_cluster()` the dedicated function of the `factoextra`
package.

```{r}
# Retrieval of the clusters
cluster_cut <- cutree(x_classif, k = 3)

# Multidimensional scaling
mds <- cmdscale(mat_dist)
df <- data.frame(mds, cluster = factor(cluster_cut))
colors <- rainbow(length(unique(df$cluster)))

plot(df$X1, df$X2,
  col = colors[df$cluster], xlab = "Coordinate 1",
  ylab = "Coordinate 2", main = "Multidimensional Scaling Plot"
)
legend("topright", legend = levels(df$cluster), fill = colors)
```

```{r}
# If your original data is 'data', use it in the following way:
fviz_cluster(list(data = X, cluster = cluster_cut), geom = "point")
```

# kmeans

```{r}
# Clustering des infividus
x_clust <- kmeans(mat_dist, centers=10)
plot(scaled_x, col = x_clust$cluster)
points(x_clust$centers, col = 1:5, pch = 8)
```

## Combien de clusters faut-il faire

## `nstart`

`nstart` permet de choisir le nombre de sets aléatoires de centres à tester. Si nous prenons `nstart = 1`, un seul set aléatoire de centres sera testé. Cela donc générer des clusters différents entre deux réalisation de kmeans. Si nous prenons `nstart = 50`, 50 sets différents seront testés. Ainsi les résultats convergeront vers les mêmes clusters, garantissant une reproductibilité des résultat entre deux itérations. Un équilibre est à trouver en fonction de la taille du jeu de données, pour un petit jeu tel que le nôtre un `nstart = 50` pourrait être trop élevé.


```{r}
x_clust_1 <- kmeans(mat_dist, centers=10, nstart=1)
tb_1 <- table(x_clust_1$cluster)

x_clust_5 <- kmeans(mat_dist, centers=10, nstart=5)
tb_5 <- table(x_clust_5$cluster)

x_clust_10 <- kmeans(mat_dist, centers=10, nstart=10)
tb_10 <- table(x_clust_10$cluster)

x_clust_25 <- kmeans(mat_dist, centers=10, nstart=25)
tb_25 <- table(x_clust_25$cluster)

x_clust_50 <- kmeans(mat_dist, centers=10, nstart=50)
tb_50 <- table(x_clust_50$cluster)
```

Différentes valeurs de nstart on été testées: 1, 5 10, 25, 50, 100. La répartition étant identique à partir de 5 donc nous allons continuer avec elle.

```{r}
loc <- cmdscale(mat_dist)
x <- loc[, 1]
y <- loc[, 2]

## note asp = 1, to ensure Euclidean distances are represented correctly
plot(x, y, type = "n", xlab = "", ylab = "", asp = 1, axes = FALSE,
     main = "cmdscale(eurodist)")
text(x, y, rownames(loc), cex = 0.6)
```

```{r}
# Clustering des individus final
plot(scaled_x, col = x_clust_5$cluster)
points(x_clust$centers, col = 1:5, pch = 8)
```
cmd
Recherche du nombre optimal de clusters

```{r}
fviz_nbclust(scaled_x, method = "wss", FUNcluster=hcut)
```

# Comparaison classification et clusters

# Classification des descripteurs

```{r}
mat_cor <- cor(X)
mat_cor_dist <- as.dist(1 - mat_cor**2)
```

```{r}
# library(gplots)
# heatmap.2(mat_cor_dist, hclustfun=hclust)
```